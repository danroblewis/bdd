#!/usr/bin/env python3
"""bdd — Behavior-Driven Development catalog manager.

Manages a catalog.json of goals, expectations, and facets.
Used by humans, Curation Claude, and Implementation Claude.
"""

import argparse
import json
import os
import subprocess
import sys

CATALOG_FILE = "catalog.json"
BDD_CONFIG_FILE = "bdd.json"

VALID_RESULTS_FORMATS = ("junit", "pytest-json", "cargo-json")
VALID_COVERAGE_FORMATS = ("coverage-json", "lcov", "lcov-dir", "cobertura")

def find_catalog():
    """Find catalog.json in current directory or parents."""
    d = os.getcwd()
    while True:
        path = os.path.join(d, CATALOG_FILE)
        if os.path.exists(path):
            return path
        parent = os.path.dirname(d)
        if parent == d:
            return os.path.join(os.getcwd(), CATALOG_FILE)
        d = parent

def load_catalog(path=None):
    path = path or find_catalog()
    if not os.path.exists(path):
        print(f"Error: No {CATALOG_FILE} found. Run 'bdd init' first.", file=sys.stderr)
        sys.exit(1)
    with open(path) as f:
        return json.load(f), path

def save_catalog(catalog, path):
    with open(path, "w") as f:
        json.dump(catalog, f, indent=2)
        f.write("\n")

def find_bdd_config():
    """Find bdd.json next to catalog.json."""
    cat_path = find_catalog()
    return os.path.join(os.path.dirname(cat_path), BDD_CONFIG_FILE)

def load_bdd_config():
    """Load and validate bdd.json."""
    config_path = find_bdd_config()
    if not os.path.exists(config_path):
        print(f"Error: No {BDD_CONFIG_FILE} found. Create one with test_command, results_format, results_file, coverage_format, coverage_file.", file=sys.stderr)
        sys.exit(1)
    with open(config_path) as f:
        config = json.load(f)
    required = ("test_command", "results_format", "results_file", "coverage_format", "coverage_file")
    missing = [k for k in required if k not in config]
    if missing:
        print(f"Error: {BDD_CONFIG_FILE} missing required fields: {', '.join(missing)}", file=sys.stderr)
        sys.exit(1)
    if config["results_format"] not in VALID_RESULTS_FORMATS:
        print(f"Error: results_format must be one of: {', '.join(VALID_RESULTS_FORMATS)}", file=sys.stderr)
        sys.exit(1)
    if config["coverage_format"] not in VALID_COVERAGE_FORMATS:
        print(f"Error: coverage_format must be one of: {', '.join(VALID_COVERAGE_FORMATS)}", file=sys.stderr)
        sys.exit(1)
    return config, config_path

# --- Result Parsers ---

def parse_junit_results(filepath):
    """Parse JUnit XML results file. Returns {test_id: 'passed'|'failed'|'skipped'}."""
    import xml.etree.ElementTree as ET
    results = {}
    tree = ET.parse(filepath)
    root = tree.getroot()
    # Handle both <testsuites><testsuite>... and <testsuite>... root
    suites = root.findall(".//testsuite") if root.tag == "testsuites" else [root]
    for suite in suites:
        for tc in suite.findall("testcase"):
            classname = tc.get("classname", "")
            name = tc.get("name", "")
            # Build test_id like pytest: tests/test_foo.py::test_bar or tests.test_foo::test_bar
            if classname:
                test_id = f"{classname}::{name}"
            else:
                test_id = name
            if tc.find("failure") is not None or tc.find("error") is not None:
                results[test_id] = "failed"
            elif tc.find("skipped") is not None:
                results[test_id] = "skipped"
            else:
                results[test_id] = "passed"
    return results

def parse_pytest_json_results(filepath):
    """Parse pytest JSON report. Returns {test_id: 'passed'|'failed'|'skipped'}."""
    with open(filepath) as f:
        data = json.load(f)
    results = {}
    for test in data.get("tests", []):
        test_id = test.get("nodeid", "")
        outcome = test.get("outcome", "")
        if outcome == "passed":
            results[test_id] = "passed"
        elif outcome in ("failed", "error"):
            results[test_id] = "failed"
        elif outcome == "skipped":
            results[test_id] = "skipped"
    return results

def parse_cargo_json_results(filepath):
    """Parse cargo test JSON output. Returns {test_id: 'passed'|'failed'|'skipped'}."""
    results = {}
    with open(filepath) as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                event = json.loads(line)
            except json.JSONDecodeError:
                continue
            if event.get("type") == "test" and event.get("event") in ("ok", "failed", "ignored"):
                name = event.get("name", "")
                ev = event["event"]
                if ev == "ok":
                    results[name] = "passed"
                elif ev == "failed":
                    results[name] = "failed"
                elif ev == "ignored":
                    results[name] = "skipped"
    return results

RESULT_PARSERS = {
    "junit": parse_junit_results,
    "pytest-json": parse_pytest_json_results,
    "cargo-json": parse_cargo_json_results,
}

# --- Test ID Matching ---

def normalize_test_id(test_id):
    """Strip extensions, normalize separators, lowercase."""
    # Remove common extensions
    for ext in (".py", ".rs", ".js", ".ts", ".go"):
        test_id = test_id.replace(ext, "")
    # Normalize path separators to dots
    test_id = test_id.replace("/", ".").replace("\\", ".")
    return test_id.lower()

def match_test_to_facet(result_ids, facet_test_id):
    """Match a facet's test field against result test IDs using 3 strategies.
    Returns (matched_result_id, status) or (None, None)."""
    if not facet_test_id:
        return None, None

    # Strategy 1: Exact match
    if facet_test_id in result_ids:
        return facet_test_id, result_ids[facet_test_id]

    # Strategy 2: Normalized match
    norm_facet = normalize_test_id(facet_test_id)
    for rid, status in result_ids.items():
        if normalize_test_id(rid) == norm_facet:
            return rid, status

    # Strategy 3: Suffix match (match on function name after ::)
    if "::" in facet_test_id:
        facet_suffix = facet_test_id.rsplit("::", 1)[-1].lower()
        for rid, status in result_ids.items():
            if "::" in rid:
                rid_suffix = rid.rsplit("::", 1)[-1].lower()
                if rid_suffix == facet_suffix:
                    return rid, status

    return None, None

def next_id(nodes, prefix):
    max_n = 0
    for n in nodes:
        if n["id"].startswith(prefix + "-"):
            try:
                num = int(n["id"].split("-", 1)[1])
                if num > max_n:
                    max_n = num
            except ValueError:
                pass
    return f"{prefix}-{max_n + 1:03d}"

TYPE_PREFIX = {"goal": "g", "expectation": "e", "facet": "f"}

def get_node(nodes, node_id):
    for n in nodes:
        if n["id"] == node_id:
            return n
    return None

def get_children(nodes, parent_id):
    return [n for n in nodes if n.get("parent") == parent_id]

def get_ancestor_chain(nodes, node_id):
    """Return list from root to node."""
    chain = []
    current = get_node(nodes, node_id)
    while current:
        chain.append(current)
        pid = current.get("parent")
        current = get_node(nodes, pid) if pid else None
    chain.reverse()
    return chain

def compute_status(nodes, node):
    """Compute status for any node from descendant facets."""
    if node["type"] == "facet":
        return node.get("status", "untested")
    children = get_children(nodes, node["id"])
    if not children:
        return "untested"
    statuses = [compute_status(nodes, c) for c in children]
    if all(s == "passing" for s in statuses):
        return "passing"
    if any(s == "failing" for s in statuses):
        return "failing"
    return "untested"

def status_icon(status):
    if status == "passing":
        return "[+]"
    if status == "failing":
        return "[-]"
    return "[ ]"

# --- Commands ---

def cmd_init(args):
    path = os.path.join(os.getcwd(), CATALOG_FILE)
    if os.path.exists(path) and not args.force:
        print(f"{CATALOG_FILE} already exists. Use --force to overwrite.", file=sys.stderr)
        sys.exit(1)
    catalog = {"version": 1, "nodes": []}
    save_catalog(catalog, path)
    print(f"Created {path}")

def cmd_status(args):
    catalog, _ = load_catalog()
    nodes = catalog["nodes"]
    goals = [n for n in nodes if n["type"] == "goal"]
    expectations = [n for n in nodes if n["type"] == "expectation"]
    facets = [n for n in nodes if n["type"] == "facet"]
    passing = [f for f in facets if f.get("status") == "passing"]
    failing = [f for f in facets if f.get("status") == "failing"]
    untested = [f for f in facets if f.get("status", "untested") == "untested"]

    total = len(facets)
    coverage = (len(passing) / total * 100) if total > 0 else 0

    satisfied = sum(1 for e in expectations if compute_status(nodes, e) == "passing")
    unsatisfied = len(expectations) - satisfied

    if args.json:
        print(json.dumps({
            "goals": len(goals),
            "expectations": len(expectations),
            "facets": total,
            "passing": len(passing),
            "failing": len(failing),
            "untested": len(untested),
            "coverage": round(coverage, 1),
            "satisfied": satisfied,
            "unsatisfied": unsatisfied,
        }))
    else:
        print(f"Goals:        {len(goals)}")
        print(f"Expectations: {len(expectations)} ({satisfied} satisfied, {unsatisfied} unsatisfied)")
        print(f"Facets:       {total}")
        print(f"  Passing:    {len(passing)}")
        print(f"  Failing:    {len(failing)}")
        print(f"  Untested:   {len(untested)}")
        print(f"Coverage:     {coverage:.0f}%")

        unsatisfied_exps = [e for e in expectations if compute_status(nodes, e) != "passing"]
        unsatisfied_exps.sort(key=lambda e: e.get("priority", 99))
        if unsatisfied_exps:
            print(f"\nTop unsatisfied ({len(unsatisfied_exps)} total):")
            for exp in unsatisfied_exps[:10]:
                parent = get_node(nodes, exp.get("parent"))
                prefix = parent["id"] if parent else "?"
                status = compute_status(nodes, exp)
                print(f"  {exp['id']} [{status}] {exp['text']}  ({prefix})")
                for f in get_children(nodes, exp["id"]):
                    if f.get("status", "untested") != "passing":
                        print(f"    {f['id']} [{f.get('status', 'untested')}] {f['text']}")

def cmd_next(args):
    catalog, _ = load_catalog()
    nodes = catalog["nodes"]
    expectations = [n for n in nodes if n["type"] == "expectation"]

    # Sort by priority (lower = higher priority), filter unsatisfied
    unsatisfied = [e for e in expectations if compute_status(nodes, e) != "passing"]
    unsatisfied.sort(key=lambda e: e.get("priority", 99))

    if not unsatisfied:
        if args.json:
            print(json.dumps(None))
        else:
            print("All expectations satisfied!")
        return

    exp = unsatisfied[0]
    facets = get_children(nodes, exp["id"])
    parent = get_node(nodes, exp.get("parent"))

    if args.json:
        print(json.dumps({
            "expectation": exp,
            "facets": facets,
            "parent_goal": parent,
        }))
    else:
        if parent:
            print(f"Goal: {parent['id']} — {parent['text']}")
            print()
        print(f"Expectation: {exp['id']} — {exp['text']}")
        if exp.get("priority"):
            print(f"Priority: {exp['priority']}")
        if exp.get("labels"):
            print(f"Labels: {', '.join(exp['labels'])}")
        print()
        if facets:
            print("Facets:")
            for f in facets:
                icon = status_icon(f.get("status", "untested"))
                test = f" (test: {f['test']})" if f.get("test") else ""
                print(f"  {icon} {f['id']} — {f['text']}{test}")
        else:
            print("No facets yet — decompose this expectation into testable facets.")

def cmd_show(args):
    catalog, _ = load_catalog()
    nodes = catalog["nodes"]
    node = get_node(nodes, args.id)
    if not node:
        print(f"Error: Node '{args.id}' not found.", file=sys.stderr)
        sys.exit(1)

    chain = get_ancestor_chain(nodes, args.id)
    children = get_children(nodes, args.id)
    status = compute_status(nodes, node)

    if args.json:
        print(json.dumps({
            "node": node,
            "status": status,
            "children": children,
            "ancestor_chain": chain,
        }))
    else:
        # Show ancestor chain
        if len(chain) > 1:
            print("Context:")
            for i, a in enumerate(chain[:-1]):
                indent = "  " * i
                print(f"  {indent}{a['id']} ({a['type']}) — {a['text']}")
            print()

        print(f"{'Type:':<10} {node['type']}")
        print(f"{'ID:':<10} {node['id']}")
        print(f"{'Text:':<10} {node['text']}")
        print(f"{'Status:':<10} {status}")
        if node.get("parent"):
            print(f"{'Parent:':<10} {node['parent']}")
        if node.get("priority"):
            print(f"{'Priority:':<10} {node['priority']}")
        if node.get("labels"):
            print(f"{'Labels:':<10} {', '.join(node['labels'])}")
        if node.get("test"):
            print(f"{'Test:':<10} {node['test']}")

        if children:
            print(f"\nChildren ({len(children)}):")
            for c in children:
                s = compute_status(nodes, c)
                icon = status_icon(s)
                print(f"  {icon} {c['id']} ({c['type']}) — {c['text']}")

def cmd_tree(args):
    catalog, _ = load_catalog()
    nodes = catalog["nodes"]
    roots = [n for n in nodes if n.get("parent") is None]
    roots.sort(key=lambda n: n.get("priority", 99))

    if args.json:
        def build_tree(node):
            children = get_children(nodes, node["id"])
            children.sort(key=lambda n: n.get("priority", 99))
            return {
                **node,
                "computed_status": compute_status(nodes, node),
                "children": [build_tree(c) for c in children],
            }
        print(json.dumps([build_tree(r) for r in roots], indent=2))
    else:
        def print_tree(node, indent=0):
            status = compute_status(nodes, node)
            icon = status_icon(status)
            prefix = "  " * indent
            type_label = node["type"][0].upper()
            print(f"{prefix}{icon} {node['id']} [{type_label}] {node['text']}")
            children = get_children(nodes, node["id"])
            children.sort(key=lambda n: n.get("priority", 99))
            for c in children:
                print_tree(c, indent + 1)

        if not roots:
            print("Catalog is empty. Use 'bdd add goal \"...\"' to get started.")
        for r in roots:
            print_tree(r)

def cmd_add(args):
    catalog, path = load_catalog()
    nodes = catalog["nodes"]
    ntype = args.type

    if ntype not in TYPE_PREFIX:
        print(f"Error: Type must be one of: goal, expectation, facet", file=sys.stderr)
        sys.exit(1)

    # Validate parent
    parent_id = args.parent
    if parent_id:
        parent = get_node(nodes, parent_id)
        if not parent:
            print(f"Error: Parent '{parent_id}' not found.", file=sys.stderr)
            sys.exit(1)
    elif ntype == "expectation":
        # Expectations must have a goal parent
        goals = [n for n in nodes if n["type"] == "goal"]
        if len(goals) == 1:
            parent_id = goals[0]["id"]
        elif len(goals) > 1:
            print("Error: Multiple goals exist. Use --parent to specify.", file=sys.stderr)
            sys.exit(1)
    elif ntype == "facet":
        # Facets must have an expectation parent
        expectations = [n for n in nodes if n["type"] == "expectation"]
        if len(expectations) == 1:
            parent_id = expectations[0]["id"]
        elif len(expectations) > 1:
            print("Error: Multiple expectations exist. Use --parent to specify.", file=sys.stderr)
            sys.exit(1)

    new_id = next_id(nodes, TYPE_PREFIX[ntype])

    node = {
        "id": new_id,
        "type": ntype,
        "text": args.text,
        "parent": parent_id,
    }

    if ntype in ("goal", "expectation"):
        node["priority"] = args.priority or 1
        node["labels"] = args.label or []
    elif ntype == "facet":
        node["test"] = None
        node["status"] = "untested"

    nodes.append(node)
    save_catalog(catalog, path)

    if args.json:
        print(json.dumps(node))
    else:
        print(f"Added {ntype}: {new_id} — {args.text}")
        if parent_id:
            print(f"  Parent: {parent_id}")

def cmd_mark(args):
    catalog, path = load_catalog()
    nodes = catalog["nodes"]
    node = get_node(nodes, args.facet_id)

    if not node:
        print(f"Error: Node '{args.facet_id}' not found.", file=sys.stderr)
        sys.exit(1)
    if node["type"] != "facet":
        print(f"Error: '{args.facet_id}' is a {node['type']}, not a facet. Only facets can be marked.", file=sys.stderr)
        sys.exit(1)

    valid = ("passing", "failing", "untested")
    if args.status not in valid:
        print(f"Error: Status must be one of: {', '.join(valid)}", file=sys.stderr)
        sys.exit(1)

    old = node.get("status", "untested")
    node["status"] = args.status
    save_catalog(catalog, path)

    if args.json:
        print(json.dumps({"id": node["id"], "old_status": old, "new_status": args.status}))
    else:
        print(f"Marked {args.facet_id}: {old} -> {args.status}")

def cmd_link(args):
    catalog, path = load_catalog()
    nodes = catalog["nodes"]
    node = get_node(nodes, args.facet_id)

    if not node:
        print(f"Error: Node '{args.facet_id}' not found.", file=sys.stderr)
        sys.exit(1)
    if node["type"] != "facet":
        print(f"Error: '{args.facet_id}' is a {node['type']}, not a facet.", file=sys.stderr)
        sys.exit(1)

    node["test"] = args.test_path
    save_catalog(catalog, path)

    if args.json:
        print(json.dumps({"id": node["id"], "test": args.test_path}))
    else:
        print(f"Linked {args.facet_id} -> {args.test_path}")

def cmd_remove(args):
    catalog, path = load_catalog()
    nodes = catalog["nodes"]
    node = get_node(nodes, args.id)

    if not node:
        print(f"Error: Node '{args.id}' not found.", file=sys.stderr)
        sys.exit(1)

    children = get_children(nodes, args.id)
    if children and not args.force:
        child_ids = [c["id"] for c in children]
        print(f"Warning: '{args.id}' has {len(children)} children: {', '.join(child_ids)}", file=sys.stderr)
        print(f"Children will be orphaned (parent set to null). Use --force to confirm.", file=sys.stderr)
        sys.exit(1)

    # Orphan children
    for c in children:
        c["parent"] = None

    catalog["nodes"] = [n for n in nodes if n["id"] != args.id]
    save_catalog(catalog, path)

    if args.json:
        print(json.dumps({"removed": args.id, "orphaned": [c["id"] for c in children]}))
    else:
        print(f"Removed {args.id}")
        if children:
            print(f"  Orphaned: {', '.join(c['id'] for c in children)}")

def cmd_edit(args):
    catalog, path = load_catalog()
    nodes = catalog["nodes"]
    node = get_node(nodes, args.id)

    if not node:
        print(f"Error: Node '{args.id}' not found.", file=sys.stderr)
        sys.exit(1)

    old = node["text"]
    node["text"] = args.text
    save_catalog(catalog, path)

    if args.json:
        print(json.dumps({"id": args.id, "old_text": old, "new_text": args.text}))
    else:
        print(f"Updated {args.id}: {args.text}")

def cmd_related(args):
    """Show motivation chain for a source file (requires coverage map)."""
    coverage_path = os.path.join(os.path.dirname(find_catalog()), "coverage_map.json")
    if not os.path.exists(coverage_path):
        if args.json:
            print(json.dumps({"error": "No coverage_map.json found. Run tests with coverage to generate it."}))
        else:
            print("No coverage_map.json found. Run tests with coverage to generate it.")
        return

    with open(coverage_path) as f:
        coverage = json.load(f)

    catalog, _ = load_catalog()
    nodes = catalog["nodes"]

    filepath = args.filepath
    # Find matching files (substring match on keys)
    matched_files = {f: lines for f, lines in coverage.items() if filepath in f}

    if not matched_files:
        if args.json:
            print(json.dumps({"file": filepath, "related": []}))
        else:
            print(f"No catalog entries related to {filepath}")
        return

    # Collect facet IDs, optionally filtering by line range
    line_range = getattr(args, 'line_range', None)
    line_start = int(line_range[0]) if line_range else None
    line_end = int(line_range[1]) if line_range else None

    results = []
    for src_file, line_map in matched_files.items():
        facet_ids = set()
        for line_num, fids in line_map.items():
            if line_start is not None and line_end is not None:
                if not (line_start <= int(line_num) <= line_end):
                    continue
            for fid in fids:
                facet_ids.add(fid)

        chains = []
        for fid in sorted(facet_ids):
            chain = get_ancestor_chain(nodes, fid)
            if chain:
                chains.append({
                    "facet_id": fid,
                    "chain": [{"id": n["id"], "type": n["type"], "text": n["text"]} for n in chain]
                })
        if facet_ids:
            results.append({"file": src_file, "facet_ids": sorted(facet_ids), "chains": chains})

    if args.json:
        print(json.dumps({"file": filepath, "related": results}))
    else:
        for r in results:
            print(f"{r['file']}:")
            for c in r["chains"]:
                chain_str = " > ".join(f"{n['id']} ({n['type']}): {n['text']}" for n in c["chain"])
                print(f"  {chain_str}")

def cmd_coverage(args):
    """Parse per-test coverage data and update coverage_map.json."""
    import xml.etree.ElementTree as ET

    catalog, cat_path = load_catalog()
    nodes = catalog["nodes"]
    project_root = os.path.dirname(cat_path)
    coverage_map_path = os.path.join(project_root, "coverage_map.json")

    # Build reverse map: test_identifier -> [facet_id]
    test_to_facets = {}
    for n in nodes:
        if n["type"] == "facet" and n.get("test"):
            test_to_facets.setdefault(n["test"], []).append(n["id"])

    # Read coverage input
    if args.dir:
        # lcov-dir mode: directory of per-test LCOV files
        fmt = "lcov-dir"
        raw = None
    elif args.file:
        with open(args.file) as f:
            raw = f.read()
    else:
        raw = sys.stdin.read()

    if not args.dir:
        fmt = args.format
        if not fmt:
            stripped = raw.strip()
            if stripped.startswith("{"):
                fmt = "coverage-json"
            elif stripped.startswith("TN:") or stripped.startswith("SF:"):
                fmt = "lcov"
            elif stripped.startswith("<?xml") or stripped.startswith("<coverage"):
                fmt = "cobertura"
            else:
                print("Error: Could not auto-detect coverage format. Use --format.", file=sys.stderr)
                sys.exit(1)

    # coverage_map: {filepath: {line_num_str: set(facet_ids)}}
    coverage_map = {}

    def add_line(filepath, line_num, facet_ids):
        """Add facet IDs for a specific line of a file."""
        filepath = os.path.relpath(filepath, project_root) if os.path.isabs(filepath) else filepath
        if filepath not in coverage_map:
            coverage_map[filepath] = {}
        line_str = str(line_num)
        if line_str not in coverage_map[filepath]:
            coverage_map[filepath][line_str] = set()
        coverage_map[filepath][line_str].update(facet_ids)

    if fmt == "coverage-json":
        # coverage.py JSON with contexts: {files: {filepath: {contexts: {context_name: [line_nums]}}}}
        # or the newer format: {files: {filepath: {executed_lines: [...], contexts: [...]}}}
        data = json.loads(raw)
        files = data.get("files", {})
        for filepath, file_data in files.items():
            contexts = file_data.get("contexts", {})
            if isinstance(contexts, dict):
                # Format: {context_name: [line_numbers]}
                for context_name, lines in contexts.items():
                    # Match context to test identifier (precise matching)
                    # coverage.py contexts look like "test_id|run" — strip suffix
                    clean_ctx = context_name.split("|")[0].strip()
                    matched_facets = []
                    # Exact match
                    if clean_ctx in test_to_facets:
                        matched_facets = test_to_facets[clean_ctx]
                    else:
                        # Normalized match
                        norm_ctx = normalize_test_id(clean_ctx)
                        for test_id, facet_ids in test_to_facets.items():
                            if normalize_test_id(test_id) == norm_ctx:
                                matched_facets.extend(facet_ids)
                                break
                        # Suffix match
                        if not matched_facets and "::" in clean_ctx:
                            ctx_suffix = clean_ctx.rsplit("::", 1)[-1].lower()
                            for test_id, facet_ids in test_to_facets.items():
                                if "::" in test_id:
                                    tid_suffix = test_id.rsplit("::", 1)[-1].lower()
                                    if tid_suffix == ctx_suffix:
                                        matched_facets.extend(facet_ids)
                                        break
                    if matched_facets:
                        for line in lines:
                            add_line(filepath, line, matched_facets)

    elif fmt == "lcov":
        # Single LCOV file — whole-suite, no per-test info
        # Map all hit lines to all facets that have linked tests
        all_facet_ids = []
        for fids in test_to_facets.values():
            all_facet_ids.extend(fids)
        all_facet_ids = list(set(all_facet_ids))

        current_file = None
        for line in raw.splitlines():
            if line.startswith("SF:"):
                current_file = line[3:].strip()
            elif line.startswith("DA:") and current_file:
                parts = line[3:].split(",")
                if len(parts) >= 2 and int(parts[1]) > 0:
                    add_line(current_file, int(parts[0]), all_facet_ids)
            elif line.startswith("end_of_record"):
                current_file = None

    elif fmt == "lcov-dir":
        # Directory of per-test LCOV files
        # Filename convention: test identifier with / replaced by __ and :: kept
        dir_path = args.dir
        for fname in os.listdir(dir_path):
            if not fname.endswith(".lcov"):
                continue
            # Derive test identifier from filename
            test_id = fname[:-5].replace("__", "/")
            facet_ids = test_to_facets.get(test_id, [])
            if not facet_ids:
                # Try fuzzy match
                for tid, fids in test_to_facets.items():
                    if test_id in tid or tid in test_id:
                        facet_ids.extend(fids)
            if not facet_ids:
                continue

            with open(os.path.join(dir_path, fname)) as f:
                lcov_data = f.read()
            current_file = None
            for line in lcov_data.splitlines():
                if line.startswith("SF:"):
                    current_file = line[3:].strip()
                elif line.startswith("DA:") and current_file:
                    parts = line[3:].split(",")
                    if len(parts) >= 2 and int(parts[1]) > 0:
                        add_line(current_file, int(parts[0]), facet_ids)
                elif line.startswith("end_of_record"):
                    current_file = None

    elif fmt == "cobertura":
        # Cobertura XML — whole-suite
        all_facet_ids = []
        for fids in test_to_facets.values():
            all_facet_ids.extend(fids)
        all_facet_ids = list(set(all_facet_ids))

        root = ET.fromstring(raw)
        for cls in root.iter("class"):
            filename = cls.get("filename", "")
            if not filename:
                continue
            for line_el in cls.iter("line"):
                line_num = line_el.get("number")
                hits = int(line_el.get("hits", "0"))
                if line_num and hits > 0:
                    add_line(filename, int(line_num), all_facet_ids)

    # Convert sets to sorted lists for JSON serialization
    serializable = {}
    for filepath in sorted(coverage_map):
        serializable[filepath] = {}
        for line_num in sorted(coverage_map[filepath], key=lambda x: int(x)):
            serializable[filepath][line_num] = sorted(coverage_map[filepath][line_num])

    with open(coverage_map_path, "w") as f:
        json.dump(serializable, f, indent=2)
        f.write("\n")

    if args.json:
        print(json.dumps({"files_mapped": len(serializable), "path": coverage_map_path}))
    else:
        total_lines = sum(len(lines) for lines in serializable.values())
        print(f"Coverage map updated: {len(serializable)} files, {total_lines} lines mapped")
        print(f"Written to {coverage_map_path}")

def cmd_test(args):
    """Run tests, parse results, update facet statuses, run coverage."""
    config, config_path = load_bdd_config()
    catalog, cat_path = load_catalog()
    nodes = catalog["nodes"]
    project_root = os.path.dirname(cat_path)

    # Step 1: Run test command
    test_cmd = config["test_command"]
    if not args.quiet and not args.json:
        print(f"Running: {test_cmd}")
    capture = args.quiet or args.json
    result = subprocess.run(test_cmd, shell=True, cwd=project_root,
                            capture_output=capture, text=True)
    test_exit_code = result.returncode

    # Step 2: Parse results file
    results_file = os.path.join(project_root, config["results_file"])
    if not os.path.exists(results_file):
        print(f"Error: Results file not found: {results_file}", file=sys.stderr)
        if not args.quiet and result.stderr:
            print(result.stderr, file=sys.stderr)
        sys.exit(1)

    parser = RESULT_PARSERS[config["results_format"]]
    test_results = parser(results_file)

    # Step 3: Match results to facets and update statuses
    updated = []
    facets = [n for n in nodes if n["type"] == "facet"]
    for facet in facets:
        if not facet.get("test"):
            continue
        matched_id, status = match_test_to_facet(test_results, facet["test"])
        if matched_id is not None:
            old_status = facet.get("status", "untested")
            if status == "passed":
                new_status = "passing"
            elif status == "failed":
                new_status = "failing"
            else:
                continue  # skip skipped tests
            if old_status != new_status:
                facet["status"] = new_status
                updated.append({"id": facet["id"], "old": old_status, "new": new_status, "test": matched_id})

    save_catalog(catalog, cat_path)

    # Step 4: Run coverage parsing
    coverage_file = os.path.join(project_root, config["coverage_file"])
    coverage_ok = False
    if os.path.exists(coverage_file):
        # Build a fake args object for cmd_coverage
        class CoverageArgs:
            pass
        cov_args = CoverageArgs()
        cov_args.file = coverage_file
        cov_args.dir = None
        cov_args.format = config["coverage_format"]
        cov_args.json = False
        # Suppress coverage output (it goes to stdout)
        old_stdout = sys.stdout
        try:
            sys.stdout = open(os.devnull, 'w')
            cmd_coverage(cov_args)
            coverage_ok = True
        except SystemExit:
            pass
        finally:
            sys.stdout.close()
            sys.stdout = old_stdout

    # Step 5: Compute summary
    # Reload catalog after save
    catalog, _ = load_catalog(cat_path)
    nodes = catalog["nodes"]
    facets = [n for n in nodes if n["type"] == "facet"]
    passing = [f for f in facets if f.get("status") == "passing"]
    failing = [f for f in facets if f.get("status") == "failing"]
    untested = [f for f in facets if f.get("status", "untested") == "untested"]
    expectations = [n for n in nodes if n["type"] == "expectation"]
    satisfied = sum(1 for e in expectations if compute_status(nodes, e) == "passing")
    all_satisfied = satisfied == len(expectations) and len(expectations) > 0

    if args.json:
        print(json.dumps({
            "test_exit_code": test_exit_code,
            "results_parsed": len(test_results),
            "facets_updated": updated,
            "passing": len(passing),
            "failing": len(failing),
            "untested": len(untested),
            "satisfied": satisfied,
            "total_expectations": len(expectations),
            "all_satisfied": all_satisfied,
            "coverage_updated": coverage_ok,
        }))
    elif not args.quiet:
        if updated:
            print()
            print("Facet updates:")
            for u in updated:
                print(f"  {u['id']}: {u['old']} -> {u['new']}")
        print()
        print(f"Results: {len(test_results)} tests parsed")
        print(f"Facets:  {len(passing)} passing, {len(failing)} failing, {len(untested)} untested")
        print(f"Expectations: {satisfied}/{len(expectations)} satisfied")
        if coverage_ok:
            print(f"Coverage map updated")
        if all_satisfied:
            print("All expectations satisfied!")

    sys.exit(0 if all_satisfied else 1)

SETUP_NODES = [
    {
        "type": "goal",
        "text": "The project is set up for BDD development",
        "priority": 0,
        "labels": ["setup"],
    },
    {
        "type": "expectation",
        "text": "Per-test coverage tooling is installed and working",
        "parent_ref": "setup_goal",
        "priority": 0,
        "labels": ["setup"],
    },
    {
        "type": "facet",
        "text": "The coverage tool for this project's language is installed",
        "parent_ref": "coverage_exp",
    },
    {
        "type": "facet",
        "text": "Running the test command produces a coverage report with non-zero hit counts on source lines (tests must call into the source crate as a library, not shell out to a separate binary)",
        "parent_ref": "coverage_exp",
    },
    {
        "type": "facet",
        "text": "bdd_test builds the index and bdd_motivation returns facet chains for source files",
        "parent_ref": "coverage_exp",
    },
    {
        "type": "facet",
        "text": "bdd.json is configured with test_command, results, and coverage settings",
        "parent_ref": "coverage_exp",
    },
    {
        "type": "expectation",
        "text": "Project details are documented in .claude/CLAUDE.md",
        "parent_ref": "setup_goal",
        "priority": 0,
        "labels": ["setup"],
    },
    {
        "type": "facet",
        "text": "Stack, build command, and test-with-coverage command are filled in",
        "parent_ref": "docs_exp",
    },
    {
        "type": "facet",
        "text": "Key paths are documented",
        "parent_ref": "docs_exp",
    },
    {
        "type": "expectation",
        "text": "Behavior test infrastructure exists and works",
        "parent_ref": "setup_goal",
        "priority": 0,
        "labels": ["setup"],
    },
    {
        "type": "facet",
        "text": "A tests/ directory exists with at least one behavior test",
        "parent_ref": "tests_exp",
    },
    {
        "type": "facet",
        "text": "The behavior test exercises the full program (not isolated units)",
        "parent_ref": "tests_exp",
    },
    {
        "type": "facet",
        "text": "The behavior test is linked to a catalog facet via bdd link",
        "parent_ref": "tests_exp",
    },
]

SETUP_RULE = """\
# Project Setup Guide

This file is read by the planning agent when working on setup expectations.
It explains how to set up per-test coverage and behavior test infrastructure.

The `bdd_test` MCP tool handles everything: running tests, parsing results,
parsing coverage, updating facet statuses, and building the index. You just
need to configure `bdd.json` so it knows your test command and output formats.

## Step 1: Detect the Project Language

Check for these files to determine the stack:
- `Cargo.toml` → Rust
- `package.json` → Node.js
- `pyproject.toml` / `setup.py` / `requirements.txt` → Python
- `go.mod` → Go

If no project files exist, ask the human what language to use, then create
the initial project structure (e.g., `cargo init`, `npm init`, etc.).

## Step 2: Install Per-Test Coverage Tooling

### Rust
```bash
rustup component add llvm-tools-preview
cargo install cargo-llvm-cov
```

### Python
```bash
pip install pytest pytest-cov
```
The `--cov-context=test` flag is critical — it records which test covered each line.

### Node.js
```bash
npm install --save-dev c8  # or nyc
```

### Go
```bash
# No additional tooling needed — go test has built-in coverage
```

## Step 3: Configure bdd.json

Edit the `bdd.json` file in the project root. It has two jobs:
1. **test_command** — a single shell command that runs tests AND produces coverage
2. **Output files** — where to find results and coverage after the command runs

The `bdd_test` MCP tool reads this config, runs `test_command`, then parses the
output files to update facet statuses and build the code-to-intent index.

### Format reference:

- `results_format`: `junit` (pytest --junitxml, cargo test), `pytest-json` (pytest-json-report), `cargo-json` (cargo test -- -Z unstable-options --format json)
- `coverage_format`: `coverage-json` (coverage.py JSON with per-test contexts), `lcov`, `lcov-dir`, `cobertura`

### Language-specific bdd.json examples:

**Rust:**
```json
{
  "test_command": "cargo llvm-cov --lcov --output-path coverage.lcov -- --format junit > results.xml",
  "results_format": "junit",
  "results_file": "results.xml",
  "coverage_format": "lcov",
  "coverage_file": "coverage.lcov"
}
```

**Python:**
```json
{
  "test_command": "pytest tests/ --cov=src --cov-context=test --cov-report=json:coverage.json --junitxml=results.xml",
  "results_format": "junit",
  "results_file": "results.xml",
  "coverage_format": "coverage-json",
  "coverage_file": "coverage.json"
}
```

**Node.js:**
```json
{
  "test_command": "npx c8 --reporter=lcov jest --ci --reporters=jest-junit",
  "results_format": "junit",
  "results_file": "junit.xml",
  "coverage_format": "lcov",
  "coverage_file": "coverage/lcov.info"
}
```

**Go:**
```json
{
  "test_command": "go test -coverprofile=coverage.out -v ./... 2>&1 | go-junit-report > results.xml",
  "results_format": "junit",
  "results_file": "results.xml",
  "coverage_format": "lcov",
  "coverage_file": "coverage.out"
}
```

## Step 4: Write the First Behavior Test

Behavior tests exercise the FULL program, not isolated units:
- For a CLI tool: invoke the binary with inputs, check stdout/stderr
- For a web app: start the server, make HTTP requests, check responses
- For a game: invoke the entry point, check initial state via introspection
- For a library: import and run the public API end-to-end

Write the test in the project's native test framework (pytest, #[test], jest, etc.).
Link it to a catalog facet with the `bdd_link` MCP tool.

## Step 5: Verify the Pipeline

Call `bdd_test()` and verify:
1. Tests run and produce results
2. Facet statuses are updated automatically
3. `.bdd/index.json` is created with the forward/reverse motivation maps
4. `bdd_motivation("some/source/file.rs")` returns facet chains

## Step 6: Fill in CLAUDE.md

Edit `.claude/CLAUDE.md` and fill in:
- **Stack**: language, framework, key libraries
- **Build**: the build command
- **Test**: configured via bdd.json, run with `bdd_test` MCP tool
- **Key Paths**: important source directories and files

## Step 7: Update settings.json Permissions

Edit `.claude/settings.json` to add project-specific Bash permissions:
- Build commands (e.g., `Bash(cargo build*)`)
- Test commands (e.g., `Bash(cargo test*)`, `Bash(cargo llvm-cov*)`)
"""

def cmd_setup(args):
    """Set up BDD framework in a project directory."""
    import shutil

    project_dir = os.path.abspath(args.project_dir or os.getcwd())
    bdd_dir = os.path.dirname(os.path.realpath(__file__))
    framework_src = os.path.join(bdd_dir, "framework")
    claude_dst = os.path.join(project_dir, ".claude")
    catalog_path = os.path.join(project_dir, CATALOG_FILE)

    if not os.path.isdir(framework_src):
        print(f"Error: Framework directory not found at {framework_src}", file=sys.stderr)
        sys.exit(1)

    # Step 1: Copy framework to .claude/
    if os.path.isdir(claude_dst):
        if not args.force:
            print(f".claude/ already exists in {project_dir}. Use --force to overwrite.", file=sys.stderr)
            sys.exit(1)
        shutil.rmtree(claude_dst)
    shutil.copytree(framework_src, claude_dst)
    print(f"Copied framework to {claude_dst}")

    # Step 2: Create or update catalog.json with setup expectations
    if os.path.exists(catalog_path):
        with open(catalog_path) as f:
            catalog = json.load(f)
        print(f"Found existing {CATALOG_FILE} ({len(catalog['nodes'])} nodes)")
    else:
        catalog = {"version": 1, "nodes": []}
        print(f"Created new {CATALOG_FILE}")

    nodes = catalog["nodes"]

    # Check if setup goal already exists
    existing_setup = [n for n in nodes if n.get("labels") and "setup" in n["labels"] and n["type"] == "goal"]
    if existing_setup:
        print(f"Setup goal already exists ({existing_setup[0]['id']}). Skipping catalog seeding.")
    else:
        # Inject setup nodes with proper IDs and parent references
        ref_map = {}  # ref_name -> actual_id

        for template in SETUP_NODES:
            ntype = template["type"]
            new_id = next_id(nodes, TYPE_PREFIX[ntype])

            node = {
                "id": new_id,
                "type": ntype,
                "text": template["text"],
            }

            # Resolve parent
            parent_ref = template.get("parent_ref")
            if parent_ref:
                node["parent"] = ref_map.get(parent_ref)
            else:
                node["parent"] = None

            if ntype in ("goal", "expectation"):
                node["priority"] = template.get("priority", 1)
                node["labels"] = template.get("labels", [])
            elif ntype == "facet":
                node["test"] = None
                node["status"] = "untested"

            nodes.append(node)

            # Store refs for child lookups
            if ntype == "goal" and "setup" in template.get("labels", []):
                ref_map["setup_goal"] = new_id
            elif ntype == "expectation" and "coverage" in template["text"].lower():
                ref_map["coverage_exp"] = new_id
            elif ntype == "expectation" and "documented" in template["text"].lower():
                ref_map["docs_exp"] = new_id
            elif ntype == "expectation" and "infrastructure" in template["text"].lower():
                ref_map["tests_exp"] = new_id

        save_catalog(catalog, catalog_path)
        setup_goal_id = ref_map.get("setup_goal", "?")
        print(f"Seeded setup expectations (goal: {setup_goal_id})")

    # Step 3: Create setup.md rule
    setup_rule_path = os.path.join(claude_dst, "rules", "setup.md")
    os.makedirs(os.path.dirname(setup_rule_path), exist_ok=True)
    with open(setup_rule_path, "w") as f:
        f.write(SETUP_RULE)
    print(f"Created {setup_rule_path}")

    # Step 4: Create bdd.json template with smart defaults
    bdd_config_path = os.path.join(project_dir, BDD_CONFIG_FILE)
    if not os.path.exists(bdd_config_path):
        # Detect language for smart defaults
        if os.path.exists(os.path.join(project_dir, "Cargo.toml")):
            config_template = {
                "test_command": "cargo llvm-cov --lcov --output-path coverage.lcov -- --format junit > results.xml",
                "results_format": "junit",
                "results_file": "results.xml",
                "coverage_format": "lcov",
                "coverage_file": "coverage.lcov",
            }
        elif os.path.exists(os.path.join(project_dir, "package.json")):
            config_template = {
                "test_command": "npx jest --ci --coverage --coverageReporters=json --reporters=jest-junit",
                "results_format": "junit",
                "results_file": "junit.xml",
                "coverage_format": "coverage-json",
                "coverage_file": "coverage/coverage-final.json",
            }
        elif any(os.path.exists(os.path.join(project_dir, f)) for f in ("pyproject.toml", "setup.py", "requirements.txt")):
            config_template = {
                "test_command": "pytest tests/ --cov=src --cov-context=test --cov-report=json:coverage.json --junitxml=results.xml",
                "results_format": "junit",
                "results_file": "results.xml",
                "coverage_format": "coverage-json",
                "coverage_file": "coverage.json",
            }
        elif os.path.exists(os.path.join(project_dir, "go.mod")):
            config_template = {
                "test_command": "go test -coverprofile=coverage.out -v ./... 2>&1 | go-junit-report > results.xml",
                "results_format": "junit",
                "results_file": "results.xml",
                "coverage_format": "lcov",
                "coverage_file": "coverage.out",
            }
        else:
            config_template = {
                "test_command": "echo 'Configure your test command'",
                "results_format": "junit",
                "results_file": "results.xml",
                "coverage_format": "coverage-json",
                "coverage_file": "coverage.json",
            }
        with open(bdd_config_path, "w") as f:
            json.dump(config_template, f, indent=2)
            f.write("\n")
        print(f"Created {bdd_config_path} (edit test_command for your project)")
    else:
        print(f"{BDD_CONFIG_FILE} already exists, skipping")

    # Step 5: Create .mcp.json pointing to bdd_server.py
    mcp_config_path = os.path.join(project_dir, ".mcp.json")
    bdd_server_path = os.path.join(bdd_dir, "bdd_server.py")
    if not os.path.exists(mcp_config_path):
        mcp_config = {
            "mcpServers": {
                "bdd-catalog": {
                    "command": "python3",
                    "args": [bdd_server_path, project_dir]
                }
            }
        }
        with open(mcp_config_path, "w") as f:
            json.dump(mcp_config, f, indent=2)
            f.write("\n")
        print(f"Created {mcp_config_path}")
    else:
        print(f".mcp.json already exists, skipping")

    # Step 6: Create .bdd/ directory, add to .gitignore
    bdd_state_dir = os.path.join(project_dir, ".bdd")
    os.makedirs(bdd_state_dir, exist_ok=True)

    gitignore_path = os.path.join(project_dir, ".gitignore")
    gitignore_entries = [".bdd/"]
    if os.path.exists(gitignore_path):
        with open(gitignore_path) as f:
            existing = f.read()
        for entry in gitignore_entries:
            if entry not in existing:
                with open(gitignore_path, "a") as f:
                    f.write(f"\n{entry}\n")
                print(f"Added {entry} to .gitignore")
    else:
        with open(gitignore_path, "w") as f:
            f.write("\n".join(gitignore_entries) + "\n")
        print(f"Created .gitignore with {', '.join(gitignore_entries)}")

    # Summary
    print()
    print("Setup complete. Next steps:")
    print("  1. cd to your project directory")
    print("  2. Edit bdd.json with your test command")
    print("  3. Run the agent loop: ./loop.sh")
    print("     Or interactively: claude")
    print("  4. The agent will pick up setup expectations first (priority 0)")
    print()
    if os.path.exists(catalog_path):
        # Show status
        with open(catalog_path) as f:
            cat = json.load(f)
        n = cat["nodes"]
        goals = len([x for x in n if x["type"] == "goal"])
        exps = len([x for x in n if x["type"] == "expectation"])
        facets = len([x for x in n if x["type"] == "facet"])
        print(f"Catalog: {goals} goals, {exps} expectations, {facets} facets")

def main():
    parser = argparse.ArgumentParser(prog="bdd", description="BDD Catalog Manager")
    parser.add_argument("--json", action="store_true", help="Output as JSON")
    sub = parser.add_subparsers(dest="command")

    # init
    p = sub.add_parser("init", help="Create empty catalog.json")
    p.add_argument("--force", action="store_true", help="Overwrite existing catalog")

    # status
    sub.add_parser("status", help="Summary of catalog")

    # next
    sub.add_parser("next", help="Next unsatisfied expectation to work on")

    # show
    p = sub.add_parser("show", help="Show details of a node")
    p.add_argument("id", help="Node ID (e.g. g-001, e-001, f-001)")

    # tree
    sub.add_parser("tree", help="Hierarchical view of catalog")

    # related
    p = sub.add_parser("related", help="Motivation chain for a source file")
    p.add_argument("filepath", help="Source file path")
    p.add_argument("--lines", nargs=2, metavar=("START", "END"), dest="line_range",
                   help="Filter to lines in range START..END")

    # add
    p = sub.add_parser("add", help="Add a node to the catalog")
    p.add_argument("type", choices=["goal", "expectation", "facet"], help="Node type")
    p.add_argument("text", help="Node text")
    p.add_argument("--parent", help="Parent node ID")
    p.add_argument("--priority", type=int, help="Priority (lower = higher)")
    p.add_argument("--label", action="append", help="Label (repeatable)")

    # mark
    p = sub.add_parser("mark", help="Update facet status")
    p.add_argument("facet_id", help="Facet ID")
    p.add_argument("status", choices=["passing", "failing", "untested"], help="New status")

    # link
    p = sub.add_parser("link", help="Link facet to test file")
    p.add_argument("facet_id", help="Facet ID")
    p.add_argument("test_path", help="Path to test file")

    # remove
    p = sub.add_parser("remove", help="Remove a node")
    p.add_argument("id", help="Node ID")
    p.add_argument("--force", action="store_true", help="Confirm removal of node with children")

    # edit
    p = sub.add_parser("edit", help="Edit node text")
    p.add_argument("id", help="Node ID")
    p.add_argument("text", help="New text")

    # setup
    p = sub.add_parser("setup", help="Set up BDD framework in a project")
    p.add_argument("project_dir", nargs="?", help="Project directory (default: current)")
    p.add_argument("--force", action="store_true", help="Overwrite existing .claude/ directory")

    # coverage
    p = sub.add_parser("coverage", help="Parse coverage data and update coverage_map.json")
    p.add_argument("--file", help="Coverage report file (default: read stdin)")
    p.add_argument("--dir", help="Directory of per-test LCOV files (lcov-dir format)")
    p.add_argument("--format", choices=["coverage-json", "lcov", "lcov-dir", "cobertura"],
                   help="Coverage format (auto-detected if not specified)")

    # test
    p = sub.add_parser("test", help="Run tests, parse results, update facets, run coverage")
    p.add_argument("--verbose", action="store_true", help="Show detailed output")
    p.add_argument("--quiet", action="store_true", help="Suppress output (except errors)")

    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        sys.exit(1)

    # Dispatch
    cmd = {
        "init": cmd_init,
        "status": cmd_status,
        "next": cmd_next,
        "show": cmd_show,
        "tree": cmd_tree,
        "related": cmd_related,
        "add": cmd_add,
        "mark": cmd_mark,
        "link": cmd_link,
        "remove": cmd_remove,
        "edit": cmd_edit,
        "coverage": cmd_coverage,
        "setup": cmd_setup,
        "test": cmd_test,
    }
    cmd[args.command](args)

if __name__ == "__main__":
    main()
