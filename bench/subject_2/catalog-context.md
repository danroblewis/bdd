# ADK Playground â€” Motivation Catalog

This document describes WHY the code in this project exists,
organized as a hierarchy of goals, expectations, and implementation facets.

## g-001: Project Management

### e-001: CRUD operations for projects including create, read, update, delete, and listing

- **f-001**: ProjectManager.create_project generates a unique 8-char UUID, constructs a default AppConfig, calls save_project, and returns the new Project. Located in backend/project_manager.py:ProjectManager.create_project. (test: `tests/test_sample_projects.py::TestProjectValidation::test_all_sample_projects_valid`)
- **f-002**: ProjectManager.get_project loads a YAML file from disk, validates it with Project.model_validate, caches the result in self._cache, and returns None if the file does not exist or fails to parse. Located in backend/project_manager.py:ProjectManager.get_project. (test: `tests/test_sample_projects.py::TestSimpleAgent::test_parses_to_project`)
- **f-003**: ProjectManager.save_project serializes a Project via model_dump(mode='json'), writes it as YAML using yaml.safe_dump, updates the in-memory cache, and triggers _save_custom_tools and _save_custom_callbacks to write Python files to disk. Located in backend/project_manager.py:ProjectManager.save_project. (test: `tests/test_callbacks.py::TestCallbackLoading::test_callback_file_created_on_disk`)
- **f-004**: ProjectManager.delete_project removes the YAML file with path.unlink, deletes the tools directory tree via shutil.rmtree, and clears the cache entry. Returns True on success. Located in backend/project_manager.py:ProjectManager.delete_project. (test: `tests/test_sample_projects.py::TestProjectValidation::test_all_sample_projects_valid`)
- **f-005**: ProjectManager.list_projects globs for *.yaml files in the projects directory, loads each with yaml.safe_load, and returns a list of dicts with id, name, and description fields. Silently skips files that fail to parse. Located in backend/project_manager.py:ProjectManager.list_projects. (test: `tests/test_sample_projects.py::TestProjectValidation::test_all_sample_projects_valid`)
- **f-011**: ProjectManager._save_custom_tools groups custom tools by module_path, creates directory structures with __init__.py files, and writes Python source files with tool code, descriptions, and state key annotations. Located in backend/project_manager.py:ProjectManager._save_custom_tools. (test: `tests/test_sample_projects.py::TestToolAgent::test_custom_tool_definition`)
- **f-012**: ProjectManager._save_custom_callbacks groups callbacks by module prefix, creates a callbacks/ directory with proper Python module structure, and writes callback code files with auto-generated headers and imports for CallbackContext and LlmResponse. Located in backend/project_manager.py:ProjectManager._save_custom_callbacks. (test: `tests/test_callbacks.py::TestCallbackLoading::test_callback_file_created_on_disk`)

### e-002: YAML import/export allows projects to be serialized to and deserialized from YAML strings

- **f-006**: ProjectManager.get_project_yaml converts the project to a dict, rewrites callback module_paths to include function names (e.g. 'callbacks.custom' becomes 'callbacks.custom.set_foo'), and returns a YAML string. Located in backend/project_manager.py:ProjectManager.get_project_yaml. (test: `tests/test_project_parsing.py::TestProjectParsing::test_callback_config_parsing`)
- **f-007**: ProjectManager.update_project_from_yaml parses a YAML string, preserves the original project_id, converts full callback paths (module.path.function_name) back to module-only paths for internal storage, validates via Project.model_validate, and saves. Located in backend/project_manager.py:ProjectManager.update_project_from_yaml. (test: `tests/test_project_parsing.py::TestProjectParsing::test_custom_callback_definition_parsing`)

### e-003: Backup system creates gzipped snapshots of projects on change and supports restore

- **f-008**: ProjectManager._backup_project computes an MD5 hash of the current YAML file, compares it to the last known hash, creates a timestamped gzipped backup only if the content changed, and calls _cleanup_old_backups to keep at most 50 backups per project. Located in backend/project_manager.py:ProjectManager._backup_project. (test: `tests/test_sample_projects.py::TestProjectValidation::test_all_sample_projects_valid`)
- **f-009**: ProjectManager.restore_backup reads a gzipped backup, parses and validates it as a Project, preserves the original project_id, and saves it as the current project. Returns the restored Project or None on failure. Located in backend/project_manager.py:ProjectManager.restore_backup. (test: `tests/test_sample_projects.py::TestProjectValidation::test_all_sample_projects_valid`)
- **f-010**: ProjectManager._backup_loop is an async background task that runs every 60 seconds, iterating over all *.yaml project files and calling _backup_project for each. Started via start_backup_service and stopped via stop_backup_service. Located in backend/project_manager.py:ProjectManager._backup_loop. (test: `tests/test_sample_projects.py::TestProjectValidation::test_all_sample_projects_valid`)

## g-002: Data Models

### e-004: Pydantic models correctly represent agent configurations including LLM, Sequential, Loop, and Parallel agent types

- **f-013**: LlmAgentConfig is a Pydantic model with type literal 'LlmAgent', supporting id, name, instruction, optional ModelConfig, output_key, include_contents, disallow_transfer flags, tools list (ToolConfig union), sub_agents list, and six callback config lists (before/after agent/model/tool). Located in backend/models.py:LlmAgentConfig. (test: `tests/test_project_parsing.py::TestProjectParsing::test_agent_config_parsing`)
- **f-014**: SequentialAgentConfig defines a non-LLM orchestrator with type 'SequentialAgent', id, name, description, sub_agents list, and before/after agent callbacks. Runs sub-agents in declared order. Located in backend/models.py:SequentialAgentConfig. (test: `tests/test_project_parsing.py::TestProjectParsing::test_sequential_agent_parsing`)
- **f-015**: LoopAgentConfig extends the non-LLM orchestrator pattern with an optional max_iterations field. Repeatedly runs its sub-agents until max_iterations is reached or an exit_loop tool is called. Located in backend/models.py:LoopAgentConfig. (test: `tests/test_project_parsing.py::TestProjectParsing::test_loop_agent_parsing`)
- **f-016**: ParallelAgentConfig defines a non-LLM orchestrator with type 'ParallelAgent' that runs all sub-agents concurrently. Supports before/after agent callbacks. Located in backend/models.py:ParallelAgentConfig. (test: `tests/test_integration.py::TestNonLlmAgentCallbacks::test_parallel_agent_callback_executed`)

### e-005: Tool configuration models cover all tool types: function, MCP, agent, builtin, and skillset

- **f-017**: ToolConfig is a Union of FunctionToolConfig (module_path reference), MCPToolConfig (wraps MCPServerConfig), AgentToolConfig (agent_id + skip_summarization), BuiltinToolConfig (name like 'google_search' or 'exit_loop'), and SkillSetToolConfig (skillset_id). Discriminated by the 'type' literal field. Located in backend/models.py:ToolConfig. (test: `tests/test_project_parsing.py::TestProjectParsing::test_builtin_tool_parsing`)
- **f-018**: MCPServerConfig defines an MCP server connection with connection_type (stdio/sse/http), command and args for stdio, url and headers for sse/http, optional tool_filter list and tool_name_prefix for filtering, and a configurable timeout. Located in backend/models.py:MCPServerConfig. (test: `tests/test_sample_projects.py::TestProjectValidation::test_all_sample_projects_valid`)

### e-006: Runtime and evaluation models correctly serialize events, sessions, and evaluation results

- **f-019**: RunEvent is a Pydantic model with timestamp, event_type (one of agent_start/end, tool_call/result, model_call/response, state_change, transfer, callback_start/end/error, user_message), agent_name, optional branch for parallel tracking, and a data dict. Located in backend/models.py:RunEvent. (test: `tests/test_runtime.py::TestTrackingPlugin::test_tracking_plugin_emits_events`)
- **f-020**: RunSession tracks a complete run with id, project_id, started_at/ended_at timestamps, status (running/completed/error), events list, final_state dict, and token_counts dict. Located in backend/models.py:RunSession. (test: `tests/test_runtime.py::TestRunAgent::test_run_agent_creates_session`)
- **f-021**: EvalSet, EvalCase, EvalInvocation, and EvalCaseResult form a hierarchy for evaluation. EvalSet has eval_cases and eval_config. EvalCase has invocations, initial_state, and expected_final_state. EvalInvocation has user_message, expected_response, expected_tool_calls with match type, and rubrics. Located in backend/models.py:EvalSet/EvalCase/EvalInvocation. (test: `tests/test_sample_projects.py::TestProjectValidation::test_all_sample_projects_valid`)
- **f-022**: ModelConfig supports provider-specific settings: provider literal (gemini/litellm/anthropic/openai/groq/together), model_name, optional api_base for LiteLLM, generation params (temperature, max_output_tokens, top_p, top_k), and retry/timeout settings. Uses populate_by_name for alias support. Located in backend/models.py:ModelConfig. (test: `tests/test_project_parsing.py::TestProjectParsing::test_model_config_parsing`)

## g-003: Code Generation

### e-007: generate_python_code produces valid, executable Python from a Project configuration

- **f-023**: generate_python_code performs a topological sort of agents (sub-agents and agent-tools before parents) to ensure correct definition order, collects required imports (Agent, SequentialAgent, LoopAgent, ParallelAgent, LiteLlm, Claude, McpToolset, etc.), and generates a complete executable script ending with an App() instantiation. Located in backend/code_generator.py:generate_python_code. (test: `tests/test_project_parsing.py::TestCodeGeneration::test_generate_code_includes_agent`)
- **f-024**: generate_agent_code generates Agent() constructor calls for LlmAgentConfig with name, model, instruction (triple-quoted), description, output_key, tools list, sub_agents, include_contents, transfer flags, and callback wrappers using _wrap_callback for instrumentation. Located in backend/code_generator.py:generate_agent_code. (test: `tests/test_project_parsing.py::TestCodeGeneration::test_generate_code_includes_agent`)
- **f-025**: generate_agent_code produces SequentialAgent(), LoopAgent(max_iterations=N), and ParallelAgent() calls for non-LLM orchestrator types, each with sub_agents list and optional before/after agent callbacks wrapped with _wrap_callback. Located in backend/code_generator.py:generate_agent_code (SequentialAgent/LoopAgent/ParallelAgent branches). (test: `tests/test_project_parsing.py::TestCodeGeneration::test_generate_sequential_agent_code`)
- **f-026**: generate_model_code produces provider-specific model instantiation: plain string for Gemini, LiteLlm() with retries/timeout for litellm/openai/groq/together, and Claude() for anthropic. Includes temperature, max_output_tokens, top_p, top_k, api_base where set. Located in backend/code_generator.py:generate_model_code. (test: `tests/test_project_parsing.py::TestCodeGeneration::test_generate_code_includes_agent`)
- **f-027**: generate_tool_code dispatches on tool type: builtin returns the tool name directly, function returns the function name, agent returns AgentTool(agent=var_name), mcp returns a sanitized variable reference, and skillset returns a skillset variable reference. Located in backend/code_generator.py:generate_tool_code. (test: `tests/test_project_parsing.py::TestCodeGeneration::test_generate_code_with_builtin_tool`)
- **f-030**: sanitize_identifier replaces any non-alphanumeric/underscore characters with underscores and prefixes with _ if the name starts with a digit, ensuring all generated variable names are valid Python identifiers. escape_triple_quoted and escape_double_quoted handle string escaping for generated code. Located in backend/code_generator.py:sanitize_identifier/escape_triple_quoted/escape_double_quoted. (test: `tests/test_project_parsing.py::TestProjectValidation::test_project_generates_valid_code`)
- **f-031**: The topological sort in generate_python_code uses a DFS with cycle guard (visiting set) to handle circular agent references. It visits sub-agents and agent-tool dependencies before the parent agent, ensuring all referenced variables are defined before use. Located in backend/code_generator.py:generate_python_code (visit_agent closure). (test: `tests/test_runtime.py::TestRunAgent::test_generated_code_includes_all_agents`)
- **f-032**: generate_python_code emits a _wrap_callback helper function that instruments sync and async callbacks with timing and event tracking via state keys (_cb_start_ and _cb_end_ prefixed). This wrapper is injected right after imports when custom_callbacks exist. Located in backend/code_generator.py:generate_python_code (callback wrapper section). (test: `tests/test_callbacks.py::TestCallbackErrors::test_callback_module_not_found_generates_code`)

### e-008: MCP and SkillSet toolset code is correctly generated with connection params and proxy injection

- **f-028**: generate_mcp_toolset_code produces McpToolset() with StdioConnectionParams for stdio servers, injecting proxy environment variables (HTTP_PROXY, HTTPS_PROXY, NO_PROXY) and Docker Chrome args for browser MCP servers (chrome-devtools-mcp). SSE servers use SseConnectionParams with url and timeout. Located in backend/code_generator.py:generate_mcp_toolset_code. (test: `tests/test_project_parsing.py::TestCodeGeneration::test_generate_loop_agent_code`)
- **f-029**: generate_skillset_code produces a KnowledgeServiceManager instance and SkillSet() constructor call with skillset_id, project_id, manager, embedding model name, search_enabled, preload_enabled, preload_top_k, and preload_min_score settings. Located in backend/code_generator.py:generate_skillset_code. (test: `tests/test_project_parsing.py::TestProjectValidation::test_project_generates_valid_code`)

## g-004: Runtime Execution

### e-009: RuntimeManager initializes correctly, manages sessions, and executes generated agent code

- **f-033**: RuntimeManager.__init__ takes a projects_dir path, creates the directory if needed, and initializes empty dicts for sessions and _running flags. The projects_dir is stored as a Path object. Located in backend/runtime.py:RuntimeManager.__init__. (test: `tests/test_runtime.py::TestRuntimeManagerInit::test_runtime_manager_creates_projects_dir`)
- **f-034**: RuntimeManager.get_session returns the RunSession for a given session_id from the in-memory sessions dict, or None if the session does not exist. Located in backend/runtime.py:RuntimeManager.get_session. (test: `tests/test_runtime.py::TestRuntimeManagerInit::test_get_nonexistent_session_returns_none`)
- **f-035**: RuntimeManager.stop_run sets _running[session_id] to False, signaling the run_agent loop to stop at the next iteration. Located in backend/runtime.py:RuntimeManager.stop_run. (test: `tests/test_runtime.py::TestRuntimeManagerInit::test_stop_run_sets_flag`)
- **f-036**: RuntimeManager._execute_generated_code calls generate_python_code to produce a script, executes it via exec() in an isolated namespace, and extracts the 'app' variable from the namespace. Returns the constructed ADK App object. Located in backend/runtime.py:RuntimeManager._execute_generated_code. (test: `tests/test_project_parsing.py::TestCodeExecution::test_execute_code_produces_app`)
- **f-037**: RuntimeManager._prepare_temp_dir creates a temporary directory for a session, writes custom tool and callback Python files from the project's custom_tools and custom_callbacks code fields, creates __init__.py files, and adds the temp dir to sys.path for import resolution. Located in backend/runtime.py:RuntimeManager._prepare_temp_dir. (test: `tests/test_project_parsing.py::TestCodeExecution::test_execute_code_produces_app`)

### e-010: TrackingPlugin emits structured events for all agent lifecycle phases

- **f-038**: TrackingPlugin.before_agent_callback creates and emits a RunEvent with event_type='agent_start', capturing the agent name and instruction. Returns None to allow agent execution to proceed. Located in backend/runtime.py:TrackingPlugin.before_agent_callback. (test: `tests/test_runtime.py::TestTrackingPlugin::test_tracking_plugin_emits_events`)
- **f-039**: TrackingPlugin.after_agent_callback emits a RunEvent with event_type='agent_end' and the agent name. Returns None. Located in backend/runtime.py:TrackingPlugin.after_agent_callback. (test: `tests/test_runtime.py::TestTrackingPlugin::test_tracking_plugin_after_agent`)
- **f-040**: TrackingPlugin.before_model_callback emits a RunEvent with event_type='model_call' including the tool_count from llm_request.tools_dict. Returns None. Located in backend/runtime.py:TrackingPlugin.before_model_callback. (test: `tests/test_runtime.py::TestTrackingPlugin::test_tracking_plugin_model_callbacks`)
- **f-041**: TrackingPlugin.before_tool_callback emits a RunEvent with event_type='tool_call', capturing tool_name and args. TrackingPlugin.after_tool_callback emits event_type='tool_result' with the result value and any state_delta. Both return None. Located in backend/runtime.py:TrackingPlugin.before_tool_callback/after_tool_callback. (test: `tests/test_runtime.py::TestTrackingPlugin::test_tracking_plugin_tool_callbacks`)
- **f-042**: TrackingPlugin._serialize_contents converts ADK Content objects to JSON-serializable dicts, handling text parts (type='text'), function_call parts (type='function_call' with name and args), function_response parts, and thought parts. Located in backend/runtime.py:TrackingPlugin._serialize_contents. (test: `tests/test_runtime.py::TestEventSerialization::test_serialize_text_content`)

### e-011: run_agent drives a full execution loop with event streaming, session creation, and service factory usage

- **f-043**: RuntimeManager.run_agent is an async generator that creates a RunSession, prepares the temp dir, executes generated code to get the App, creates session/memory/artifact services via factory functions, runs the ADK Runner, yields RunEvents through the event_callback, and cleans up on completion or error. Located in backend/runtime.py:RuntimeManager.run_agent. (test: `tests/test_integration.py::TestCallbackExecution::test_before_agent_callback_executed`)
- **f-044**: create_session_service_from_uri dispatches on URI scheme: memory:// returns InMemorySessionService, sqlite:// returns SqliteSessionService, postgresql:///mysql:// returns DatabaseSessionService, agentengine:// returns VertexAiSessionService, file:// returns FileSessionService. Falls back to InMemorySessionService on import error. Located in backend/runtime.py:create_session_service_from_uri. (test: `tests/test_runtime.py::TestSessionManagement::test_list_sessions_from_service`)

## g-005: API Routes

### e-012: FastAPI routes expose project CRUD, agent management, tool management, and run endpoints

- **f-045**: GET /api/projects returns project_manager.list_projects(). POST /api/projects creates via project_manager.create_project(name, description). GET /api/projects/{id} loads via project_manager.get_project(id) and raises 404 if not found. DELETE /api/projects/{id} deletes via project_manager.delete_project(id). Located in backend/main.py (project CRUD routes). (test: `tests/test_sample_projects.py::TestProjectValidation::test_all_sample_projects_valid`)
- **f-046**: PUT /api/projects/{id} accepts a full project JSON body, validates it as a Project model, and calls project_manager.save_project. Returns the updated project. Located in backend/main.py (PUT /api/projects/{id}). (test: `tests/test_sample_projects.py::TestProjectValidation::test_all_sample_projects_valid`)
- **f-047**: GET/PUT /api/projects/{id}/yaml endpoints export and import projects as YAML strings using project_manager.get_project_yaml and update_project_from_yaml respectively. Located in backend/main.py (YAML routes). (test: `tests/test_project_parsing.py::TestProjectParsing::test_callback_config_parsing`)
- **f-048**: Agent CRUD routes: GET /api/projects/{id}/agents lists all agents, POST creates a new agent (LlmAgent, SequentialAgent, LoopAgent, or ParallelAgent based on the type field), PUT /api/projects/{id}/agents/{agent_id} updates an agent, DELETE removes an agent. All persist via project_manager.save_project. Located in backend/main.py (agent routes). (test: `tests/test_project_parsing.py::TestProjectParsing::test_agent_config_parsing`)
- **f-049**: Tool CRUD routes: GET /api/projects/{id}/tools lists custom tools, POST /api/projects/{id}/tools creates a new CustomToolDefinition, PUT updates tool code/metadata, DELETE removes a tool and its references from agents. Located in backend/main.py (tool routes). (test: `tests/test_sample_projects.py::TestToolAgent::test_tool_configuration`)

### e-013: WebSocket and HTTP run endpoints stream agent execution events in real-time

- **f-050**: WebSocket endpoint /ws/run accepts a connection, receives a JSON message with project_id and user_message, loads the project, and streams RunEvents from runtime_manager.run_agent as JSON over the WebSocket. Handles disconnection and errors gracefully. Located in backend/main.py (WebSocket /ws/run). (test: `tests/test_integration.py::TestCallbackExecution::test_before_agent_callback_executed`)
- **f-051**: POST /api/run is an HTTP alternative to the WebSocket that accepts project_id, user_message, and optional session_id, runs the agent synchronously via runtime_manager.run_agent, collects all events, and returns the complete list of RunEvents. Located in backend/main.py (POST /api/run). (test: `tests/test_integration.py::TestCallbackExecution::test_before_agent_callback_executed`)
- **f-052**: MCPConnectionPool manages persistent MCP server connections with a dict-based cache keyed by server configuration (command+args for stdio, url for sse/http). Includes async locking, last-access tracking, and a cleanup task to close idle connections. Located in backend/main.py:MCPConnectionPool. (test: `tests/test_sample_projects.py::TestProjectValidation::test_all_sample_projects_valid`)

### e-014: API provides model listing and MCP tool discovery endpoints

- **f-053**: GET /api/models/list calls list_all_models to query each provider (Gemini, Anthropic, OpenAI, Groq, Together, Ollama) in parallel, returning ProviderModels with available model info. Located in backend/main.py (GET /api/models/list). (test: `tests/test_sample_projects.py::TestSimpleAgent::test_model_configuration`)
- **f-054**: POST /api/mcp/tools accepts an MCP server config, creates a toolset via MCPConnectionPool.get_toolset, calls get_tools() to discover available tools, and returns their declarations as JSON. POST /api/mcp/call invokes a specific tool and returns the result. Located in backend/main.py (MCP routes). (test: `tests/test_sample_projects.py::TestProjectValidation::test_all_sample_projects_valid`)
- **f-055**: AI-assisted generation endpoints: POST /api/ai/generate-agent uses an LLM to generate agent configurations from a natural language description. POST /api/ai/generate-tool generates custom tool code. These call backend/agent_runner.py:run_agent with appropriate prompts. Located in backend/main.py (AI generation routes). (test: `tests/test_sample_projects.py::TestProjectValidation::test_all_sample_projects_valid`)
- **f-056**: Backup routes: GET /api/projects/{id}/backups lists available backups via project_manager.list_backups. POST /api/projects/{id}/backups/{filename}/restore restores a project from a backup via project_manager.restore_backup. Located in backend/main.py (backup routes). (test: `tests/test_sample_projects.py::TestProjectValidation::test_all_sample_projects_valid`)

## g-006: Evaluation System

### e-015: EvaluationService runs eval sets with ROUGE scoring and trajectory matching

- **f-057**: EvaluationService.run_eval_set iterates over each EvalCase in an EvalSet, runs it via run_eval_case, collects EvalCaseResults, computes overall pass rate and per-metric average scores, and returns an EvalSetResult with timing information. Located in backend/evaluation_service.py:EvaluationService.run_eval_set. (test: `tests/test_sample_projects.py::TestProjectValidation::test_all_sample_projects_valid`)
- **f-058**: EvaluationService.run_eval_case runs each invocation in an EvalCase sequentially via _run_invocation, collecting InvocationResults. Checks expected_final_state against actual state if configured. Aggregates metric_results and determines pass/fail. Located in backend/evaluation_service.py:EvaluationService.run_eval_case. (test: `tests/test_sample_projects.py::TestProjectValidation::test_all_sample_projects_valid`)
- **f-059**: RougeScorer implements ROUGE-1 unigram overlap scoring with optional Porter stemming via NLTK. Tokenizes text into lowercase words, counts overlap using Counter, and computes precision, recall, and F1. Used by ResponseEvaluator for fuzzy text matching. Located in backend/evaluation_service.py:RougeScorer. (test: `tests/test_sample_projects.py::TestProjectValidation::test_all_sample_projects_valid`)
- **f-060**: TrajectoryEvaluator supports three match modes: EXACT (ordered exact match of tool calls), IN_ORDER (expected tools must appear in order but not necessarily consecutively), and ANY_ORDER (all expected tools must appear regardless of order). Scores between 0.0 and 1.0. Located in backend/evaluation_service.py:TrajectoryEvaluator. (test: `tests/test_sample_projects.py::TestProjectValidation::test_all_sample_projects_valid`)
- **f-066**: ResponseEvaluator uses RougeScorer to compute ROUGE-1 F1 between expected and actual responses, then compares against the configured threshold. Returns a MetricResult with score, threshold, passed flag, and optional details. Located in backend/evaluation_service.py:ResponseEvaluator. (test: `tests/test_sample_projects.py::TestProjectValidation::test_all_sample_projects_valid`)

### e-016: LLM-as-judge evaluation supports custom rubrics and prebuilt ADK metrics

- **f-061**: _run_llm_judge sends evaluation prompts to a judge model (default gemini-2.5-flash) with the actual response, expected response, and rubric text, then parses the structured score and rationale. Supports safety, hallucination, and rubric-based quality metrics. Located in backend/evaluation_service.py:_run_llm_judge. (test: `tests/test_sample_projects.py::TestProjectValidation::test_all_sample_projects_valid`)
- **f-062**: AdkEvaluationService bridges to ADK native evaluation by converting playground EvalCase/EvalInvocation models to ADK format via _convert_to_adk_invocation and _convert_to_adk_eval_case, running ADK evaluation, and converting results back via _convert_adk_result_to_ours. Located in backend/adk_evaluation_service.py:AdkEvaluationService. (test: `tests/test_sample_projects.py::TestProjectValidation::test_all_sample_projects_valid`)

### e-017: Eval set CRUD and run endpoints are exposed through the API

- **f-063**: GET/POST/PUT/DELETE /api/projects/{id}/eval-sets manage eval sets with full CRUD. POST /api/projects/{id}/eval-sets/{eval_set_id}/run triggers execution via EvaluationService or AdkEvaluationService based on configuration. Results are streamed back as EvalSetResult. Located in backend/main.py (eval-set routes). (test: `tests/test_sample_projects.py::TestProjectValidation::test_all_sample_projects_valid`)
- **f-064**: EvalConfig holds the list of enabled EvalMetricConfig items (each with metric type, threshold, and optional judge_model_options), default_trajectory_match_type, num_runs for repeated execution, and judge_model override. Used by both EvaluationService and AdkEvaluationService. Located in backend/models.py:EvalConfig. (test: `tests/test_sample_projects.py::TestProjectValidation::test_all_sample_projects_valid`)
- **f-065**: EvalSetResult aggregates case_results with total_cases, passed_cases, failed_cases, error_cases, per-metric pass_rates and avg_scores, overall_pass_rate, and timing. EvalCaseResult includes invocation_results, final_state comparison, and per-invocation token usage. Located in backend/models.py:EvalSetResult/EvalCaseResult. (test: `tests/test_sample_projects.py::TestProjectValidation::test_all_sample_projects_valid`)

## g-007: Model Service

### e-018: Model service discovers and lists available models from multiple LLM providers

- **f-067**: list_gemini_models uses google.genai.Client to enumerate models, filters out embedding and non-chat models, extracts display_name, description, context_window (input_token_limit), and capability flags (supports_tools, supports_vision, supports_json_mode). Returns ProviderModels. Located in backend/model_service.py:list_gemini_models. (test: `tests/test_sample_projects.py::TestSimpleAgent::test_model_configuration`)
- **f-068**: list_anthropic_models, list_openai_models, list_groq_models, list_together_models each query their respective provider APIs with the appropriate API key from environment. They return ProviderModels with model metadata including context window size and capability flags. Located in backend/model_service.py:list_*_models. (test: `tests/test_sample_projects.py::TestSimpleAgent::test_model_configuration`)
- **f-069**: list_ollama_models queries the local Ollama HTTP API at the configured base URL (defaulting to http://localhost:11434), lists running and available models, and returns them as ProviderModels. Handles connection errors gracefully. Located in backend/model_service.py:list_ollama_models. (test: `tests/test_sample_projects.py::TestSimpleAgent::test_model_configuration`)
- **f-070**: list_all_models runs all provider listing functions in parallel using asyncio.gather, collects ProviderModels from each, and returns the combined list. Errors from individual providers are captured in the ProviderModels.error field. Located in backend/model_service.py:list_all_models. (test: `tests/test_sample_projects.py::TestSimpleAgent::test_model_configuration`)
- **f-071**: ModelInfo is a Pydantic model with id (API model name), human-readable name, provider string, description, optional context_window int, and boolean flags: supports_tools, supports_vision, supports_json_mode, supports_streaming. Used as the return type in all list functions. Located in backend/model_service.py:ModelInfo. (test: `tests/test_sample_projects.py::TestSimpleAgent::test_model_configuration`)
- **f-072**: ProviderModels wraps a provider name, list of ModelInfo objects, and an optional error string. This structure allows the API to return partial results when some providers are unavailable. Located in backend/model_service.py:ProviderModels. (test: `tests/test_sample_projects.py::TestSimpleAgent::test_model_configuration`)

## g-008: MCP Integration

### e-019: Known MCP servers are loaded from configuration and can be discovered by the UI

- **f-073**: load_mcp_servers_from_file reads the mcp.json file (path from ADK_PLAYGROUND_MCP_CONFIG env or ~/.adk-playground/mcp.json), parses the standard 'mcpServers' dict format, converts each entry to MCPServerConfig with auto-detected connection_type (stdio if command present, sse/http if url present), and returns the list. Located in backend/known_mcp_servers.py:load_mcp_servers_from_file. (test: `tests/test_sample_projects.py::TestProjectValidation::test_all_sample_projects_valid`)
- **f-074**: KNOWN_MCP_SERVERS is a module-level list of pre-configured MCPServerConfig objects for popular MCP servers. BUILTIN_TOOLS is a list of built-in ADK tool names (google_search, exit_loop, load_memory). Both are imported by main.py for the tool discovery API. Located in backend/known_mcp_servers.py:KNOWN_MCP_SERVERS/BUILTIN_TOOLS. (test: `tests/test_sample_projects.py::TestProjectValidation::test_all_sample_projects_valid`)
- **f-080**: MCPToolConfig wraps an MCPServerConfig and is part of the ToolConfig union type. When an agent has an MCP tool, code generation produces McpToolset() with the appropriate connection params and the agent references the toolset variable. Located in backend/models.py:MCPToolConfig / backend/code_generator.py:generate_tool_code. (test: `tests/test_sample_projects.py::TestProjectValidation::test_all_sample_projects_valid`)

### e-020: MCP connection pool manages persistent connections with timeout and cleanup

- **f-075**: MCPConnectionPool.get_toolset creates or retrieves a cached MCPToolset for a server config. Uses async locking to prevent concurrent creation of the same connection. Supports stdio (StdioConnectionParams) and sse (SseConnectionParams) connection types. Located in backend/main.py:MCPConnectionPool.get_toolset. (test: `tests/test_sample_projects.py::TestProjectValidation::test_all_sample_projects_valid`)
- **f-076**: MCPConnectionPool._get_server_key generates a unique cache key from server config: 'stdio:{command}:{args}' for stdio or '{type}:{url}' for sse/http. This ensures separate connections for different server configurations. Located in backend/main.py:MCPConnectionPool._get_server_key. (test: `tests/test_sample_projects.py::TestProjectValidation::test_all_sample_projects_valid`)
- **f-077**: generate_mcp_toolset_code detects browser MCP servers via _is_browser_mcp_server (checks server name and args for browser-related keywords) and _is_chrome_devtools_mcp (checks for chrome-devtools-mcp in args). Injects Docker Chrome args (--no-sandbox, --disable-dev-shm-usage, --headless, etc.) for container compatibility. Located in backend/code_generator.py:_is_browser_mcp_server/_is_chrome_devtools_mcp. (test: `tests/test_project_parsing.py::TestProjectValidation::test_project_generates_valid_code`)
- **f-078**: POST /api/mcp/tools endpoint accepts a server config dict, uses MCPConnectionPool to get or create a toolset, calls get_tools() to discover available tool declarations, and returns them as JSON. Handles connection timeouts via the server config timeout field. Located in backend/main.py (POST /api/mcp/tools). (test: `tests/test_sample_projects.py::TestProjectValidation::test_all_sample_projects_valid`)
- **f-079**: POST /api/mcp/call invokes a specific MCP tool by name with provided arguments through the MCPConnectionPool. It finds the tool in the cached toolset, calls it with the given args, and returns the result. Located in backend/main.py (POST /api/mcp/call). (test: `tests/test_sample_projects.py::TestProjectValidation::test_all_sample_projects_valid`)

## g-009: Knowledge and SkillSet System

### e-021: KnowledgeServiceManager and SkillSetStore provide vector-based semantic search for agent knowledge

- **f-081**: SkillSetStore.add takes a text string, generates an embedding via _embed (using google.genai EmbedContentConfig), creates a KnowledgeEntry with a hash-based ID, and appends it to the in-memory entries list. Persists to disk as JSON. Located in backend/knowledge_service.py:SkillSetStore.add. (test: `tests/test_sample_projects.py::TestProjectValidation::test_all_sample_projects_valid`)
- **f-082**: SkillSetStore.search embeds the query text, computes cosine_similarity against all stored entries, filters by min_score, sorts by descending score, and returns the top_k SearchResult objects. Located in backend/knowledge_service.py:SkillSetStore.search. (test: `tests/test_sample_projects.py::TestProjectValidation::test_all_sample_projects_valid`)
- **f-083**: cosine_similarity computes the dot product of two numpy vectors divided by the product of their norms. Returns 0.0 if either vector has zero norm. Used by SkillSetStore.search for ranking results. Located in backend/knowledge_service.py:cosine_similarity. (test: `tests/test_sample_projects.py::TestProjectValidation::test_all_sample_projects_valid`)
- **f-084**: chunk_text splits long text into overlapping chunks of a configurable size (default 500 chars) with overlap (default 100 chars), suitable for embedding. Used when adding long documents. fetch_url_content retrieves and extracts text from URLs using aiohttp. Located in backend/knowledge_service.py:chunk_text/fetch_url_content. (test: `tests/test_sample_projects.py::TestProjectValidation::test_all_sample_projects_valid`)
- **f-087**: KnowledgeServiceManager manages multiple named SkillSetStore instances per project, creating and caching stores on demand. Stores are persisted to ~/.adk-playground/skillsets/{project_id}/{skillset_id}/ as JSON files. Located in backend/knowledge_service.py:KnowledgeServiceManager. (test: `tests/test_sample_projects.py::TestProjectValidation::test_all_sample_projects_valid`)
- **f-088**: SkillSetStore._embed and _embed_batch use google.genai Client with EmbedContentConfig to generate embeddings. _embed handles single texts, _embed_batch processes multiple texts efficiently. Both fall back gracefully if google-genai is not installed (EMBEDDINGS_AVAILABLE flag). Located in backend/knowledge_service.py:SkillSetStore._embed/_embed_batch. (test: `tests/test_sample_projects.py::TestProjectValidation::test_all_sample_projects_valid`)

### e-022: SkillSet toolset integrates with ADK agents for knowledge-augmented generation

- **f-085**: SearchSkillSetTool extends BaseTool with a run_async method that takes a 'query' arg, calls SkillSetStore.search, and returns formatted results as a string. The _get_declaration method returns a FunctionDeclaration for the search tool. Located in backend/skillset.py:SearchSkillSetTool. (test: `tests/test_sample_projects.py::TestProjectValidation::test_all_sample_projects_valid`)
- **f-086**: SkillSet extends BaseToolset with get_tools (returns SearchSkillSetTool) and process_llm_request for knowledge preloading. When preload_enabled is True, it searches the knowledge store using the user message, injects relevant context into the LLM request system instruction. Located in backend/skillset.py:SkillSet. (test: `tests/test_sample_projects.py::TestProjectValidation::test_all_sample_projects_valid`)

## g-010: Sandbox and Docker

### e-023: SandboxManager controls Docker container lifecycle for isolated agent execution

- **f-089**: SandboxManager.start_sandbox creates a gateway container (mitmproxy), an agent runner container, and optional MCP server containers based on the SandboxConfig. It builds or pulls required Docker images, sets up networking, and configures volume mounts for storage services. Located in backend/sandbox/docker_manager.py:SandboxManager.start_sandbox. (test: `tests/test_sample_projects.py::TestProjectValidation::test_all_sample_projects_valid`)
- **f-090**: SandboxManager.stop_sandbox stops and removes all containers associated with a sandbox (gateway, runner, MCP). Cleans up Docker networks and temporary files. Located in backend/sandbox/docker_manager.py:SandboxManager.stop_sandbox. (test: `tests/test_sample_projects.py::TestProjectValidation::test_all_sample_projects_valid`)
- **f-091**: SandboxManager.get_sandbox_status returns a SandboxStatus with container states (running/stopped/error) for the gateway, runner, and MCP containers. Queries Docker API for each container. Located in backend/sandbox/docker_manager.py:SandboxManager.get_sandbox_status. (test: `tests/test_sample_projects.py::TestProjectValidation::test_all_sample_projects_valid`)
- **f-092**: SandboxManager.send_message_to_agent sends a user message to the running agent container via HTTP, receives streamed RunEvents, and forwards them to the caller. Located in backend/sandbox/docker_manager.py:SandboxManager.send_message_to_agent. (test: `tests/test_sample_projects.py::TestProjectValidation::test_all_sample_projects_valid`)

### e-024: Sandbox configuration model and persistence handles network allowlists and container settings

- **f-093**: SandboxConfig defines the full sandbox configuration including network_allowlist (NetworkAllowlist with AllowlistPattern entries), volume_mounts, environment variables, resource limits, and MCP server configs. Located in backend/sandbox/models.py:SandboxConfig. (test: `tests/test_sample_projects.py::TestProjectValidation::test_all_sample_projects_valid`)
- **f-094**: allowlist_persistence module provides load/save functions for sandbox configuration as YAML files. Handles migration from older formats and validates AllowlistPattern entries. Located in backend/sandbox/allowlist_persistence.py. (test: `tests/test_sample_projects.py::TestProjectValidation::test_all_sample_projects_valid`)
- **f-095**: extract_storage_paths_from_project inspects session_service_uri, memory_service_uri, and artifact_service_uri for file:// and sqlite:// schemes, creates VolumeMount objects to bind-mount those paths into the container. Located in backend/sandbox/docker_manager.py:extract_storage_paths_from_project. (test: `tests/test_sample_projects.py::TestProjectValidation::test_memory_service_uris`)
- **f-096**: MCPContainerManager handles lifecycle of MCP server containers within the sandbox: creating containers with appropriate connection params, health checking, restarting on failure. Webhook handler (webhook_handler.py) receives MCP events via HTTP. Located in backend/sandbox/mcp_manager.py:MCPContainerManager / backend/sandbox/webhook_handler.py. (test: `tests/test_sample_projects.py::TestProjectValidation::test_all_sample_projects_valid`)

## g-011: Error Handling and Resilience

### e-025: ADK errors are parsed into actionable messages with helpful hints

- **f-097**: parse_adk_error uses regex matching to categorize common ADK errors: missing state variables ('Context variable not found'), missing artifacts, unknown tools, missing agents, rate limits, auth errors, and timeouts. Returns a dict with message, hint, and error_type. Located in backend/runtime.py:parse_adk_error. (test: `tests/test_runtime.py::TestTrackingPlugin::test_tracking_plugin_emits_events`)
- **f-098**: extract_exception_details handles Python 3.11+ ExceptionGroup (from asyncio.TaskGroup) by iterating sub-exceptions, parsing each with parse_adk_error, and combining messages and hints. Captures full stack traces for each sub-error. Located in backend/runtime.py:extract_exception_details. (test: `tests/test_runtime.py::TestTrackingPlugin::test_tracking_plugin_emits_events`)

### e-026: Callback and tool errors are captured gracefully without crashing the runtime

- **f-099**: Callback error events include error message, error_type, and stack_trace in the RunEvent data dict. The _wrap_callback function generated in code_generator.py catches exceptions during callback execution and records timing information via state keys. Located in backend/code_generator.py (callback wrapper) / backend/runtime.py (error handling). (test: `tests/test_callbacks.py::TestCallbackErrors::test_callback_error_captured`)
- **f-100**: Callback module reloading: RuntimeManager removes callback modules from sys.modules before each run to ensure fresh imports. This allows live code updates without restarting the server. TestCallbackModuleReloading verifies that updated callback code is reflected after reimport. Located in backend/runtime.py (module cleanup) / tests/test_callbacks.py:TestCallbackModuleReloading. (test: `tests/test_callbacks.py::TestCallbackModuleReloading::test_module_removed_from_sys_modules`)
- **f-101**: Projects with missing or invalid callbacks still generate code successfully. The code_generator falls back to using the full module_path as a string reference when callback definitions cannot be resolved, allowing the code to be generated even if execution would later fail. Located in backend/code_generator.py:generate_agent_code (callback fallback logic). (test: `tests/test_callbacks.py::TestCallbackErrors::test_callback_module_not_found_generates_code`)
