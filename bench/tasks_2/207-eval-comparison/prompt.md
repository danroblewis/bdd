Add eval run comparison. Add a `compare_eval_results(result_a: EvalSetResult, result_b: EvalSetResult) -> dict` function to `evaluation_service.py` that compares two eval set results and returns a diff showing: which cases improved, which degraded, which are unchanged, and overall score delta. Add a `POST /api/projects/{project_id}/eval-sets/{eval_set_id}/compare` endpoint that accepts `{"result_a_id": str, "result_b_id": str}`.
