Fix a bug in the evaluation metric aggregation. When `run_eval_set` in `evaluation_service.py` runs multiple eval cases, the `overall_score` in the returned `EvalSetResult` should be the average of all case scores (weighted by number of invocations per case), but currently there's no aggregation â€” the overall_score field is missing. Add `overall_score` computation to `EvalSetResult` and ensure `run_eval_set` populates it.
